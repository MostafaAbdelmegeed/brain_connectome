{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphIO\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "import gcn_model\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"data/GNN_Benchmark/\"\n",
    "MAT_DIR = DATASET_DIR + \"matrices2326/\"\n",
    "DATASET = DATASET_DIR + \"data/Thickness_Swapped_wID.xlsx\"\n",
    "MAPPINGS_DIR = DATASET_DIR + \"data/region_name_mapping.json\"\n",
    "ID_COL = 'ID'\n",
    "LABEL_COL = 'DX'\n",
    "RANDOM_STATE = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = graphIO.read_adj_matrices_from_directory(MAT_DIR)\n",
    "mappings = graphIO.read_mappings_from_json(MAPPINGS_DIR)\n",
    "ids = list(matrices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ID_COL] + \\\n",
    "    [f'Node {node_idx}' for node_idx in range(1, 161)] + [LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graphIO.read_nodes_from_excel(\n",
    "    DATASET, columns=columns, ids_col=ID_COL, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes.loc[nodes[LABEL_COL].isin(['AD', 'CN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[LABEL_COL] = nodes[LABEL_COL].map({'AD': 1, 'CN': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DX\n",
       "0    844\n",
       "1    240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "DX\n",
      "0    844\n",
      "1    844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "# Perform oversampling\n",
    "X, labels = oversampler.fit_resample(\n",
    "    nodes.iloc[:, :-1], nodes.iloc[:, -1])\n",
    "\n",
    "# Print the class distribution after oversampling (optional)\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(pd.Series(labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node features\n",
    "graph_ids = X[ID_COL].values\n",
    "node_features = X.drop(columns=[ID_COL]).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs shape: (1688,), Node features shape: (1688, 160), Labels shape: (1688,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'IDs shape: {graph_ids.shape}, Node features shape: {node_features.shape}, Labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training, validation, and test sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     node_features, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your GCN model\n",
    "# Example dimensions, replace with your actual values\n",
    "input_dim = len(node_features[1])\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "model = gcn_model.GCN(input_dim, hidden_dim, output_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "def create_pyg_data(node_features, adj_matrix, labels):\n",
    "    # Convert node features, adjacency matrix, and labels to PyTorch tensors\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    # Assuming your adjacency matrix is in COO format\n",
    "    edge_index = torch.tensor(adj_matrix, dtype=torch.long)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    # Create a PyTorch Geometric data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'941_S_6094_I921886'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pyg_data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(node_features)):\n\u001b[0;32m      4\u001b[0m     graph_data \u001b[38;5;241m=\u001b[39m create_pyg_data(\n\u001b[1;32m----> 5\u001b[0m         node_features[i], nx\u001b[38;5;241m.\u001b[39madjacency_matrix(\u001b[43mmatrices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgraph_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mgetnnz(), labels[i])\n\u001b[0;32m      6\u001b[0m     pyg_data_list\u001b[38;5;241m.\u001b[39mappend(graph_data)\n",
      "\u001b[1;31mKeyError\u001b[0m: '941_S_6094_I921886'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "pyg_data_list = []\n",
    "for i in range(len(node_features)):\n",
    "    graph_data = create_pyg_data(\n",
    "        node_features[i], nx.adjacency_matrix(matrices[graph_ids[i]]).getnnz(), labels[i])\n",
    "    pyg_data_list.append(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(pyg_data_list, [int(\n",
    "    0.8 * len(pyg_data_list)), len(pyg_data_list) - int(0.8 * len(pyg_data_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            val_loss += criterion(out, data.y).item()\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {(correct/total)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
