{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('C:/Users/mosta/OneDrive - UNCG\\Academics/CSC 699 - Thesis/repos/brain_connectome/graphIO')\n",
    "from graphIO import read_ad_adj_data, read_ad_curv_data, analyze_matrices\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_ADJ_DIR = \"C:/Users/mosta/OneDrive - UNCG/Academics/CSC 699 - Thesis/data/ad_adjacencies/\"\n",
    "AD_CURV_DIR = \"C:/Users/mosta/OneDrive - UNCG/Academics/CSC 699 - Thesis/data/curvatures/\"\n",
    "ATLAS = 160\n",
    "HIDDEN_DIM = 96\n",
    "LATENT_DIM = 80\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 100\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading adjacency matrices: 100%|██████████| 50/50 [00:00<00:00, 322.52it/s]\n",
      "Reading adjacency matrices: 100%|██████████| 50/50 [00:00<00:00, 357.90it/s]\n",
      "Reading adjacency matrices: 100%|██████████| 50/50 [00:00<00:00, 311.54it/s]\n",
      "Reading adjacency matrices: 100%|██████████| 50/50 [00:00<00:00, 308.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for the entire set of matrices:\n",
      "Mean: 2.6867397195928787e-17\n",
      "Standard Deviation: 1.0000000000000002\n",
      "Maximum Value: 45.895410203419495\n",
      "Minimum Value: -0.1663877963580864\n",
      "----------------------------------------\n",
      "Statistics for the entire set of matrices:\n",
      "Mean: 1.2079226507921702e-17\n",
      "Standard Deviation: 1.0\n",
      "Maximum Value: 5.524465098710106\n",
      "Minimum Value: -0.4229439417607101\n",
      "----------------------------------------\n",
      "Statistics for the entire set of matrices:\n",
      "Mean: 1.1793899190593038e-16\n",
      "Standard Deviation: 0.9999999999999999\n",
      "Maximum Value: 43.91243325526468\n",
      "Minimum Value: -0.1657311204532757\n",
      "----------------------------------------\n",
      "Statistics for the entire set of matrices:\n",
      "Mean: 9.952039192739903e-17\n",
      "Standard Deviation: 1.0000000000000004\n",
      "Maximum Value: 5.716892641685452\n",
      "Minimum Value: -0.4064465658939247\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "control_adj_matrices, patient_adj_matrices = read_ad_adj_data(AD_ADJ_DIR)\n",
    "control_curv_matrices, patient_curv_matrices = read_ad_curv_data(AD_CURV_DIR)\n",
    "\n",
    "analyze_matrices(control_adj_matrices)\n",
    "analyze_matrices(control_curv_matrices)\n",
    "analyze_matrices(patient_adj_matrices)\n",
    "analyze_matrices(patient_curv_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 160, 160]) torch.Size([50, 1, 160, 160]) torch.Size([50, 1, 160, 160]) torch.Size([50, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "tensors = {}\n",
    "tensors['control_adj'] = torch.tensor(control_adj_matrices.reshape((-1, ATLAS, ATLAS)), dtype=torch.float32).unsqueeze(1)\n",
    "tensors['patient_adj'] = torch.tensor(patient_adj_matrices.reshape((-1, ATLAS, ATLAS)), dtype=torch.float32).unsqueeze(1)\n",
    "tensors['control_curv'] = torch.tensor(control_curv_matrices.reshape((-1, ATLAS, ATLAS)), dtype=torch.float32).unsqueeze(1)\n",
    "tensors['patient_curv'] = torch.tensor(patient_curv_matrices.reshape((-1, ATLAS, ATLAS)), dtype=torch.float32).unsqueeze(1)\n",
    "print(tensors['control_adj'].shape, tensors['patient_adj'].shape, tensors['control_curv'].shape, tensors['patient_curv'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape=(1, ATLAS, ATLAS), hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # output shape: (32, ATLAS, ATLAS)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),  # output shape: (64, ATLAS, ATLAS)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # output shape: (128, ATLAS, ATLAS)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()  # output shape: (128 * ATLAS * ATLAS)\n",
    "        )\n",
    "        \n",
    "        self.flattened_size = 128 * ATLAS * ATLAS\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.fc4 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, self.flattened_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (128, ATLAS, ATLAS)),\n",
    "            nn.ConvTranspose2d(128, 64, 3, padding=1),  # output shape: (64, ATLAS, ATLAS)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, padding=1),  # output shape: (32, ATLAS, ATLAS)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, padding=1),  # output shape: (1, ATLAS, ATLAS)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = torch.relu(self.bn1(self.fc1(h)))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.bn2(self.fc4(z)))\n",
    "        h = torch.relu(self.fc5(h))\n",
    "        h = self.decoder(h)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "def calculate_mse(dataloader, model):\n",
    "    model.eval()\n",
    "    mse_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(DEVICE)\n",
    "            recon, _, _ = model(data)\n",
    "            mse_loss += nn.functional.mse_loss(recon, data, reduction='sum').item()\n",
    "    return mse_loss / len(dataloader.dataset)\n",
    "\n",
    "# Function to get the latent space representation\n",
    "def get_latent_space(model, data_vector, use_mean=True):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        # Ensure the data is in the correct shape and tensor format\n",
    "        data_tensor = torch.tensor(data_vector.reshape(1, 1, ATLAS, ATLAS), dtype=torch.float32).to(DEVICE)\n",
    "        # Pass through the encoder to get mu and logvar\n",
    "        mu, logvar = model.encode(data_tensor)\n",
    "        if use_mean:\n",
    "            return mu.cpu().numpy()  # Return the mean as the latent representation\n",
    "        else:\n",
    "            # Sample from the distribution using reparameterization trick\n",
    "            z = model.reparameterize(mu, logvar)\n",
    "            return z.cpu().numpy()  # Return the sampled latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, dataloader, epochs=100, learning_rate=1e-3, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the Variational Autoencoder (VAE) model.\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): The VAE model to train.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    epochs (int): Number of training epochs.\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    device (str): Device to run the training on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data)\n",
    "            loss = loss_function(recon, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print the loss for each batch\n",
    "            print(f\"Epoch {epoch + 1} [{batch_idx + 1}/{len(dataloader)}], Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate time taken for the epoch\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        \n",
    "        # Print the average loss for this epoch and time taken\n",
    "        avg_loss = train_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Estimate and print the remaining time\n",
    "        remaining_time = epoch_time * (epochs - epoch - 1)\n",
    "        print(f\"Estimated remaining time: {remaining_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_patient_curv_tensor = torch.concatenate((tensors['control_curv'], tensors['patient_curv']))\n",
    "control_patient_curv_dataset = TensorDataset(control_patient_curv_tensor, control_patient_curv_tensor)\n",
    "control_patient_curv_dataloader = DataLoader(control_patient_curv_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [1/4], Batch Loss: 1278663.2500\n",
      "Epoch 1 [2/4], Batch Loss: 1060325.2500\n",
      "Epoch 1 [3/4], Batch Loss: 928902.3750\n",
      "Epoch 1 [4/4], Batch Loss: 117643.0625\n",
      "Epoch 1, Average Loss: 33855.3394, Time: 41.35s\n",
      "Estimated remaining time: 68.24 minutes\n",
      "Epoch 2 [1/4], Batch Loss: 786527.1250\n",
      "Epoch 2 [2/4], Batch Loss: 760852.0625\n",
      "Epoch 2 [3/4], Batch Loss: 698770.6250\n",
      "Epoch 2 [4/4], Batch Loss: 88073.3203\n",
      "Epoch 2, Average Loss: 23342.2313, Time: 38.42s\n",
      "Estimated remaining time: 62.76 minutes\n",
      "Epoch 3 [1/4], Batch Loss: 671608.8750\n",
      "Epoch 3 [2/4], Batch Loss: 592227.6250\n",
      "Epoch 3 [3/4], Batch Loss: 616674.1250\n",
      "Epoch 3 [4/4], Batch Loss: 78631.7188\n",
      "Epoch 3, Average Loss: 19591.4234, Time: 37.99s\n",
      "Estimated remaining time: 61.42 minutes\n",
      "Epoch 4 [1/4], Batch Loss: 555930.2500\n",
      "Epoch 4 [2/4], Batch Loss: 581783.4375\n",
      "Epoch 4 [3/4], Batch Loss: 589822.4375\n",
      "Epoch 4 [4/4], Batch Loss: 72479.2969\n",
      "Epoch 4, Average Loss: 18000.1542, Time: 39.78s\n",
      "Estimated remaining time: 63.65 minutes\n",
      "Epoch 5 [1/4], Batch Loss: 541946.9375\n",
      "Epoch 5 [2/4], Batch Loss: 551108.0625\n",
      "Epoch 5 [3/4], Batch Loss: 550638.0000\n",
      "Epoch 5 [4/4], Batch Loss: 78550.5000\n",
      "Epoch 5, Average Loss: 17222.4350, Time: 38.36s\n",
      "Estimated remaining time: 60.74 minutes\n",
      "Epoch 6 [1/4], Batch Loss: 529846.0625\n",
      "Epoch 6 [2/4], Batch Loss: 516613.1562\n",
      "Epoch 6 [3/4], Batch Loss: 531629.4375\n",
      "Epoch 6 [4/4], Batch Loss: 66280.9688\n",
      "Epoch 6, Average Loss: 16443.6963, Time: 37.53s\n",
      "Estimated remaining time: 58.79 minutes\n",
      "Epoch 7 [1/4], Batch Loss: 516892.7500\n",
      "Epoch 7 [2/4], Batch Loss: 545221.8750\n",
      "Epoch 7 [3/4], Batch Loss: 497899.4688\n",
      "Epoch 7 [4/4], Batch Loss: 58940.7852\n",
      "Epoch 7, Average Loss: 16189.5488, Time: 43.24s\n",
      "Estimated remaining time: 67.02 minutes\n",
      "Epoch 8 [1/4], Batch Loss: 495878.6250\n",
      "Epoch 8 [2/4], Batch Loss: 524731.4375\n",
      "Epoch 8 [3/4], Batch Loss: 490425.0000\n",
      "Epoch 8 [4/4], Batch Loss: 70274.4062\n",
      "Epoch 8, Average Loss: 15813.0947, Time: 38.61s\n",
      "Estimated remaining time: 59.20 minutes\n",
      "Epoch 9 [1/4], Batch Loss: 505801.3750\n",
      "Epoch 9 [2/4], Batch Loss: 482583.4688\n",
      "Epoch 9 [3/4], Batch Loss: 495565.4688\n",
      "Epoch 9 [4/4], Batch Loss: 74689.2891\n",
      "Epoch 9, Average Loss: 15586.3960, Time: 40.96s\n",
      "Estimated remaining time: 62.12 minutes\n",
      "Epoch 10 [1/4], Batch Loss: 497122.5625\n",
      "Epoch 10 [2/4], Batch Loss: 497449.0000\n",
      "Epoch 10 [3/4], Batch Loss: 496862.8750\n",
      "Epoch 10 [4/4], Batch Loss: 50927.7852\n",
      "Epoch 10, Average Loss: 15423.6222, Time: 41.66s\n",
      "Estimated remaining time: 62.49 minutes\n",
      "Epoch 11 [1/4], Batch Loss: 495400.1250\n",
      "Epoch 11 [2/4], Batch Loss: 470472.7188\n",
      "Epoch 11 [3/4], Batch Loss: 503757.5000\n",
      "Epoch 11 [4/4], Batch Loss: 58358.9570\n",
      "Epoch 11, Average Loss: 15279.8930, Time: 57.70s\n",
      "Estimated remaining time: 85.58 minutes\n",
      "Epoch 12 [1/4], Batch Loss: 483456.1875\n",
      "Epoch 12 [2/4], Batch Loss: 500381.1875\n",
      "Epoch 12 [3/4], Batch Loss: 467450.6875\n",
      "Epoch 12 [4/4], Batch Loss: 64746.9297\n",
      "Epoch 12, Average Loss: 15160.3499, Time: 38.21s\n",
      "Estimated remaining time: 56.04 minutes\n",
      "Epoch 13 [1/4], Batch Loss: 489916.2188\n",
      "Epoch 13 [2/4], Batch Loss: 452475.3125\n",
      "Epoch 13 [3/4], Batch Loss: 517604.4062\n",
      "Epoch 13 [4/4], Batch Loss: 56771.9609\n",
      "Epoch 13, Average Loss: 15167.6790, Time: 38.08s\n",
      "Estimated remaining time: 55.22 minutes\n",
      "Epoch 14 [1/4], Batch Loss: 469310.6875\n",
      "Epoch 14 [2/4], Batch Loss: 481607.2500\n",
      "Epoch 14 [3/4], Batch Loss: 485065.4375\n",
      "Epoch 14 [4/4], Batch Loss: 73903.5469\n",
      "Epoch 14, Average Loss: 15098.8692, Time: 40.56s\n",
      "Estimated remaining time: 58.13 minutes\n",
      "Epoch 15 [1/4], Batch Loss: 454155.4688\n",
      "Epoch 15 [2/4], Batch Loss: 477139.1875\n",
      "Epoch 15 [3/4], Batch Loss: 493249.6562\n",
      "Epoch 15 [4/4], Batch Loss: 76037.5000\n",
      "Epoch 15, Average Loss: 15005.8181, Time: 42.96s\n",
      "Estimated remaining time: 60.85 minutes\n",
      "Epoch 16 [1/4], Batch Loss: 475659.7812\n",
      "Epoch 16 [2/4], Batch Loss: 512057.8750\n",
      "Epoch 16 [3/4], Batch Loss: 439854.1875\n",
      "Epoch 16 [4/4], Batch Loss: 70734.6641\n",
      "Epoch 16, Average Loss: 14983.0651, Time: 37.71s\n",
      "Estimated remaining time: 52.79 minutes\n",
      "Epoch 17 [1/4], Batch Loss: 492895.1562\n",
      "Epoch 17 [2/4], Batch Loss: 445995.7812\n",
      "Epoch 17 [3/4], Batch Loss: 502057.7812\n",
      "Epoch 17 [4/4], Batch Loss: 56130.1758\n",
      "Epoch 17, Average Loss: 14970.7889, Time: 39.92s\n",
      "Estimated remaining time: 55.22 minutes\n",
      "Epoch 18 [1/4], Batch Loss: 474543.6250\n",
      "Epoch 18 [2/4], Batch Loss: 472217.8750\n",
      "Epoch 18 [3/4], Batch Loss: 487698.0000\n",
      "Epoch 18 [4/4], Batch Loss: 55267.2695\n",
      "Epoch 18, Average Loss: 14897.2677, Time: 39.11s\n",
      "Estimated remaining time: 53.45 minutes\n",
      "Epoch 19 [1/4], Batch Loss: 478888.9375\n",
      "Epoch 19 [2/4], Batch Loss: 468186.0312\n",
      "Epoch 19 [3/4], Batch Loss: 495279.8125\n",
      "Epoch 19 [4/4], Batch Loss: 47222.4648\n",
      "Epoch 19, Average Loss: 14895.7725, Time: 38.80s\n",
      "Estimated remaining time: 52.38 minutes\n",
      "Epoch 20 [1/4], Batch Loss: 489455.8750\n",
      "Epoch 20 [2/4], Batch Loss: 471867.3438\n",
      "Epoch 20 [3/4], Batch Loss: 467983.3125\n",
      "Epoch 20 [4/4], Batch Loss: 56247.7070\n",
      "Epoch 20, Average Loss: 14855.5424, Time: 37.50s\n",
      "Estimated remaining time: 49.99 minutes\n",
      "Epoch 21 [1/4], Batch Loss: 491783.7188\n",
      "Epoch 21 [2/4], Batch Loss: 455145.0000\n",
      "Epoch 21 [3/4], Batch Loss: 473326.8438\n",
      "Epoch 21 [4/4], Batch Loss: 65043.8594\n",
      "Epoch 21, Average Loss: 14852.9942, Time: 37.44s\n",
      "Estimated remaining time: 49.30 minutes\n",
      "Epoch 22 [1/4], Batch Loss: 463241.1875\n",
      "Epoch 22 [2/4], Batch Loss: 486531.4062\n",
      "Epoch 22 [3/4], Batch Loss: 482918.8125\n",
      "Epoch 22 [4/4], Batch Loss: 50660.9180\n",
      "Epoch 22, Average Loss: 14833.5232, Time: 43.30s\n",
      "Estimated remaining time: 56.29 minutes\n",
      "Epoch 23 [1/4], Batch Loss: 487658.1250\n",
      "Epoch 23 [2/4], Batch Loss: 459396.6875\n",
      "Epoch 23 [3/4], Batch Loss: 480230.9375\n",
      "Epoch 23 [4/4], Batch Loss: 55408.3711\n",
      "Epoch 23, Average Loss: 14826.9412, Time: 37.48s\n",
      "Estimated remaining time: 48.11 minutes\n",
      "Epoch 24 [1/4], Batch Loss: 509867.0625\n",
      "Epoch 24 [2/4], Batch Loss: 445324.4375\n",
      "Epoch 24 [3/4], Batch Loss: 475047.7188\n",
      "Epoch 24 [4/4], Batch Loss: 51921.3672\n",
      "Epoch 24, Average Loss: 14821.6059, Time: 37.42s\n",
      "Estimated remaining time: 47.39 minutes\n",
      "Epoch 25 [1/4], Batch Loss: 444626.8750\n",
      "Epoch 25 [2/4], Batch Loss: 462998.8750\n",
      "Epoch 25 [3/4], Batch Loss: 503939.0000\n",
      "Epoch 25 [4/4], Batch Loss: 67384.8359\n",
      "Epoch 25, Average Loss: 14789.4959, Time: 41.22s\n",
      "Estimated remaining time: 51.53 minutes\n",
      "Epoch 26 [1/4], Batch Loss: 496307.0000\n",
      "Epoch 26 [2/4], Batch Loss: 487577.4688\n",
      "Epoch 26 [3/4], Batch Loss: 434803.0938\n",
      "Epoch 26 [4/4], Batch Loss: 58743.7891\n",
      "Epoch 26, Average Loss: 14774.3135, Time: 39.09s\n",
      "Estimated remaining time: 48.22 minutes\n",
      "Epoch 27 [1/4], Batch Loss: 464675.1875\n",
      "Epoch 27 [2/4], Batch Loss: 493886.8125\n",
      "Epoch 27 [3/4], Batch Loss: 463315.8125\n",
      "Epoch 27 [4/4], Batch Loss: 57579.2500\n",
      "Epoch 27, Average Loss: 14794.5706, Time: 39.12s\n",
      "Estimated remaining time: 47.60 minutes\n",
      "Epoch 28 [1/4], Batch Loss: 467676.6562\n",
      "Epoch 28 [2/4], Batch Loss: 469597.6875\n",
      "Epoch 28 [3/4], Batch Loss: 472523.9375\n",
      "Epoch 28 [4/4], Batch Loss: 62678.9023\n",
      "Epoch 28, Average Loss: 14724.7718, Time: 37.34s\n",
      "Estimated remaining time: 44.80 minutes\n",
      "Epoch 29 [1/4], Batch Loss: 457597.2812\n",
      "Epoch 29 [2/4], Batch Loss: 461774.6250\n",
      "Epoch 29 [3/4], Batch Loss: 485989.0625\n",
      "Epoch 29 [4/4], Batch Loss: 70540.8906\n",
      "Epoch 29, Average Loss: 14759.0186, Time: 37.87s\n",
      "Estimated remaining time: 44.81 minutes\n",
      "Epoch 30 [1/4], Batch Loss: 505778.0938\n",
      "Epoch 30 [2/4], Batch Loss: 431907.3125\n",
      "Epoch 30 [3/4], Batch Loss: 472885.2812\n",
      "Epoch 30 [4/4], Batch Loss: 62361.8984\n",
      "Epoch 30, Average Loss: 14729.3259, Time: 38.46s\n",
      "Estimated remaining time: 44.87 minutes\n",
      "Epoch 31 [1/4], Batch Loss: 464566.1562\n",
      "Epoch 31 [2/4], Batch Loss: 465987.4062\n",
      "Epoch 31 [3/4], Batch Loss: 475612.3125\n",
      "Epoch 31 [4/4], Batch Loss: 65339.6406\n",
      "Epoch 31, Average Loss: 14715.0552, Time: 37.50s\n",
      "Estimated remaining time: 43.13 minutes\n",
      "Epoch 32 [1/4], Batch Loss: 452232.0938\n",
      "Epoch 32 [2/4], Batch Loss: 483314.5625\n",
      "Epoch 32 [3/4], Batch Loss: 466735.2500\n",
      "Epoch 32 [4/4], Batch Loss: 66766.6250\n",
      "Epoch 32, Average Loss: 14690.4853, Time: 38.09s\n",
      "Estimated remaining time: 43.17 minutes\n",
      "Epoch 33 [1/4], Batch Loss: 458661.7812\n",
      "Epoch 33 [2/4], Batch Loss: 465985.3750\n",
      "Epoch 33 [3/4], Batch Loss: 486519.3750\n",
      "Epoch 33 [4/4], Batch Loss: 55389.8047\n",
      "Epoch 33, Average Loss: 14665.5634, Time: 37.42s\n",
      "Estimated remaining time: 41.78 minutes\n",
      "Epoch 34 [1/4], Batch Loss: 471614.0000\n",
      "Epoch 34 [2/4], Batch Loss: 451966.1562\n",
      "Epoch 34 [3/4], Batch Loss: 490082.0625\n",
      "Epoch 34 [4/4], Batch Loss: 52998.5664\n",
      "Epoch 34, Average Loss: 14666.6079, Time: 38.68s\n",
      "Estimated remaining time: 42.55 minutes\n",
      "Epoch 35 [1/4], Batch Loss: 463848.8438\n",
      "Epoch 35 [2/4], Batch Loss: 463655.6562\n",
      "Epoch 35 [3/4], Batch Loss: 476986.3125\n",
      "Epoch 35 [4/4], Batch Loss: 59561.0625\n",
      "Epoch 35, Average Loss: 14640.5187, Time: 38.70s\n",
      "Estimated remaining time: 41.92 minutes\n",
      "Epoch 36 [1/4], Batch Loss: 446791.5312\n",
      "Epoch 36 [2/4], Batch Loss: 495458.2500\n",
      "Epoch 36 [3/4], Batch Loss: 459114.7500\n",
      "Epoch 36 [4/4], Batch Loss: 60314.7891\n",
      "Epoch 36, Average Loss: 14616.7932, Time: 36.80s\n",
      "Estimated remaining time: 39.25 minutes\n",
      "Epoch 37 [1/4], Batch Loss: 469211.3750\n",
      "Epoch 37 [2/4], Batch Loss: 447354.5938\n",
      "Epoch 37 [3/4], Batch Loss: 490165.6562\n",
      "Epoch 37 [4/4], Batch Loss: 55854.3242\n",
      "Epoch 37, Average Loss: 14625.8595, Time: 40.55s\n",
      "Estimated remaining time: 42.58 minutes\n",
      "Epoch 38 [1/4], Batch Loss: 485177.8125\n",
      "Epoch 38 [2/4], Batch Loss: 455775.9375\n",
      "Epoch 38 [3/4], Batch Loss: 453948.1562\n",
      "Epoch 38 [4/4], Batch Loss: 69586.5234\n",
      "Epoch 38, Average Loss: 14644.8843, Time: 36.30s\n",
      "Estimated remaining time: 37.51 minutes\n",
      "Epoch 39 [1/4], Batch Loss: 444064.1562\n",
      "Epoch 39 [2/4], Batch Loss: 477104.9688\n",
      "Epoch 39 [3/4], Batch Loss: 487138.4062\n",
      "Epoch 39 [4/4], Batch Loss: 52747.0078\n",
      "Epoch 39, Average Loss: 14610.5454, Time: 37.94s\n",
      "Estimated remaining time: 38.57 minutes\n",
      "Epoch 40 [1/4], Batch Loss: 458877.6875\n",
      "Epoch 40 [2/4], Batch Loss: 478112.9688\n",
      "Epoch 40 [3/4], Batch Loss: 463624.8125\n",
      "Epoch 40 [4/4], Batch Loss: 56612.4648\n",
      "Epoch 40, Average Loss: 14572.2793, Time: 45.45s\n",
      "Estimated remaining time: 45.45 minutes\n",
      "Epoch 41 [1/4], Batch Loss: 468808.5312\n",
      "Epoch 41 [2/4], Batch Loss: 457943.5312\n",
      "Epoch 41 [3/4], Batch Loss: 464051.8750\n",
      "Epoch 41 [4/4], Batch Loss: 66688.2031\n",
      "Epoch 41, Average Loss: 14574.9214, Time: 37.50s\n",
      "Estimated remaining time: 36.87 minutes\n",
      "Epoch 42 [1/4], Batch Loss: 457540.6562\n",
      "Epoch 42 [2/4], Batch Loss: 459226.4688\n",
      "Epoch 42 [3/4], Batch Loss: 492089.8438\n",
      "Epoch 42 [4/4], Batch Loss: 50081.4531\n",
      "Epoch 42, Average Loss: 14589.3842, Time: 38.94s\n",
      "Estimated remaining time: 37.64 minutes\n",
      "Epoch 43 [1/4], Batch Loss: 474097.6875\n",
      "Epoch 43 [2/4], Batch Loss: 455457.5312\n",
      "Epoch 43 [3/4], Batch Loss: 478496.7500\n",
      "Epoch 43 [4/4], Batch Loss: 50081.7344\n",
      "Epoch 43, Average Loss: 14581.3370, Time: 41.77s\n",
      "Estimated remaining time: 39.68 minutes\n",
      "Epoch 44 [1/4], Batch Loss: 451380.6875\n",
      "Epoch 44 [2/4], Batch Loss: 484031.6562\n",
      "Epoch 44 [3/4], Batch Loss: 469583.4688\n",
      "Epoch 44 [4/4], Batch Loss: 52111.5742\n",
      "Epoch 44, Average Loss: 14571.0739, Time: 44.03s\n",
      "Estimated remaining time: 41.09 minutes\n",
      "Epoch 45 [1/4], Batch Loss: 482639.6562\n",
      "Epoch 45 [2/4], Batch Loss: 469566.5625\n",
      "Epoch 45 [3/4], Batch Loss: 441178.9062\n",
      "Epoch 45 [4/4], Batch Loss: 61756.7969\n",
      "Epoch 45, Average Loss: 14551.4192, Time: 37.01s\n",
      "Estimated remaining time: 33.93 minutes\n",
      "Epoch 46 [1/4], Batch Loss: 446813.9375\n",
      "Epoch 46 [2/4], Batch Loss: 456489.5938\n",
      "Epoch 46 [3/4], Batch Loss: 492128.7500\n",
      "Epoch 46 [4/4], Batch Loss: 59268.5273\n",
      "Epoch 46, Average Loss: 14547.0081, Time: 39.20s\n",
      "Estimated remaining time: 35.28 minutes\n",
      "Epoch 47 [1/4], Batch Loss: 466896.3438\n",
      "Epoch 47 [2/4], Batch Loss: 483918.1250\n",
      "Epoch 47 [3/4], Batch Loss: 450221.0312\n",
      "Epoch 47 [4/4], Batch Loss: 52295.1289\n",
      "Epoch 47, Average Loss: 14533.3063, Time: 37.75s\n",
      "Estimated remaining time: 33.35 minutes\n",
      "Epoch 48 [1/4], Batch Loss: 468937.3125\n",
      "Epoch 48 [2/4], Batch Loss: 461516.6875\n",
      "Epoch 48 [3/4], Batch Loss: 467623.7188\n",
      "Epoch 48 [4/4], Batch Loss: 52414.4609\n",
      "Epoch 48, Average Loss: 14504.9218, Time: 37.15s\n",
      "Estimated remaining time: 32.20 minutes\n",
      "Epoch 49 [1/4], Batch Loss: 441079.7812\n",
      "Epoch 49 [2/4], Batch Loss: 467072.7188\n",
      "Epoch 49 [3/4], Batch Loss: 484464.2188\n",
      "Epoch 49 [4/4], Batch Loss: 59950.9102\n",
      "Epoch 49, Average Loss: 14525.6763, Time: 39.06s\n",
      "Estimated remaining time: 33.20 minutes\n",
      "Epoch 50 [1/4], Batch Loss: 474504.3125\n",
      "Epoch 50 [2/4], Batch Loss: 473289.6875\n",
      "Epoch 50 [3/4], Batch Loss: 439286.4688\n",
      "Epoch 50 [4/4], Batch Loss: 63871.0391\n",
      "Epoch 50, Average Loss: 14509.5151, Time: 36.89s\n",
      "Estimated remaining time: 30.74 minutes\n",
      "Epoch 51 [1/4], Batch Loss: 466637.3750\n",
      "Epoch 51 [2/4], Batch Loss: 468498.2188\n",
      "Epoch 51 [3/4], Batch Loss: 452719.6250\n",
      "Epoch 51 [4/4], Batch Loss: 62060.0625\n",
      "Epoch 51, Average Loss: 14499.1528, Time: 39.23s\n",
      "Estimated remaining time: 32.04 minutes\n",
      "Epoch 52 [1/4], Batch Loss: 477504.6562\n",
      "Epoch 52 [2/4], Batch Loss: 446953.5000\n",
      "Epoch 52 [3/4], Batch Loss: 460362.4375\n",
      "Epoch 52 [4/4], Batch Loss: 64231.3086\n",
      "Epoch 52, Average Loss: 14490.5190, Time: 41.37s\n",
      "Estimated remaining time: 33.10 minutes\n",
      "Epoch 53 [1/4], Batch Loss: 458051.3750\n",
      "Epoch 53 [2/4], Batch Loss: 457478.2188\n",
      "Epoch 53 [3/4], Batch Loss: 470786.0625\n",
      "Epoch 53 [4/4], Batch Loss: 58506.0117\n",
      "Epoch 53, Average Loss: 14448.2167, Time: 37.30s\n",
      "Estimated remaining time: 29.22 minutes\n",
      "Epoch 54 [1/4], Batch Loss: 466851.1875\n",
      "Epoch 54 [2/4], Batch Loss: 456008.7188\n",
      "Epoch 54 [3/4], Batch Loss: 464872.8750\n",
      "Epoch 54 [4/4], Batch Loss: 58612.7656\n",
      "Epoch 54, Average Loss: 14463.4555, Time: 37.70s\n",
      "Estimated remaining time: 28.91 minutes\n",
      "Epoch 55 [1/4], Batch Loss: 472055.4375\n",
      "Epoch 55 [2/4], Batch Loss: 436778.2500\n",
      "Epoch 55 [3/4], Batch Loss: 474460.4375\n",
      "Epoch 55 [4/4], Batch Loss: 63494.6211\n",
      "Epoch 55, Average Loss: 14467.8875, Time: 39.65s\n",
      "Estimated remaining time: 29.74 minutes\n",
      "Epoch 56 [1/4], Batch Loss: 471474.6562\n",
      "Epoch 56 [2/4], Batch Loss: 475515.6875\n",
      "Epoch 56 [3/4], Batch Loss: 445126.4062\n",
      "Epoch 56 [4/4], Batch Loss: 52402.2891\n",
      "Epoch 56, Average Loss: 14445.1904, Time: 43.35s\n",
      "Estimated remaining time: 31.79 minutes\n",
      "Epoch 57 [1/4], Batch Loss: 449969.4062\n",
      "Epoch 57 [2/4], Batch Loss: 457457.4688\n",
      "Epoch 57 [3/4], Batch Loss: 485360.6562\n",
      "Epoch 57 [4/4], Batch Loss: 50934.9258\n",
      "Epoch 57, Average Loss: 14437.2246, Time: 37.63s\n",
      "Estimated remaining time: 26.96 minutes\n",
      "Epoch 58 [1/4], Batch Loss: 424043.6562\n",
      "Epoch 58 [2/4], Batch Loss: 498525.8750\n",
      "Epoch 58 [3/4], Batch Loss: 458400.3438\n",
      "Epoch 58 [4/4], Batch Loss: 64620.5234\n",
      "Epoch 58, Average Loss: 14455.9040, Time: 37.96s\n",
      "Estimated remaining time: 26.57 minutes\n",
      "Epoch 59 [1/4], Batch Loss: 439678.6875\n",
      "Epoch 59 [2/4], Batch Loss: 452700.2188\n",
      "Epoch 59 [3/4], Batch Loss: 485235.5938\n",
      "Epoch 59 [4/4], Batch Loss: 64719.6094\n",
      "Epoch 59, Average Loss: 14423.3411, Time: 36.71s\n",
      "Estimated remaining time: 25.09 minutes\n",
      "Epoch 60 [1/4], Batch Loss: 437583.8750\n",
      "Epoch 60 [2/4], Batch Loss: 488556.5000\n",
      "Epoch 60 [3/4], Batch Loss: 454389.5000\n",
      "Epoch 60 [4/4], Batch Loss: 60098.3203\n",
      "Epoch 60, Average Loss: 14406.2820, Time: 38.48s\n",
      "Estimated remaining time: 25.65 minutes\n",
      "Epoch 61 [1/4], Batch Loss: 429382.3125\n",
      "Epoch 61 [2/4], Batch Loss: 469014.5000\n",
      "Epoch 61 [3/4], Batch Loss: 489952.3750\n",
      "Epoch 61 [4/4], Batch Loss: 54931.9297\n",
      "Epoch 61, Average Loss: 14432.8112, Time: 37.43s\n",
      "Estimated remaining time: 24.33 minutes\n",
      "Epoch 62 [1/4], Batch Loss: 447116.0312\n",
      "Epoch 62 [2/4], Batch Loss: 455073.2188\n",
      "Epoch 62 [3/4], Batch Loss: 470498.0938\n",
      "Epoch 62 [4/4], Batch Loss: 65091.3555\n",
      "Epoch 62, Average Loss: 14377.7870, Time: 37.13s\n",
      "Estimated remaining time: 23.52 minutes\n",
      "Epoch 63 [1/4], Batch Loss: 467019.4062\n",
      "Epoch 63 [2/4], Batch Loss: 448454.0938\n",
      "Epoch 63 [3/4], Batch Loss: 464680.4375\n",
      "Epoch 63 [4/4], Batch Loss: 57575.9648\n",
      "Epoch 63, Average Loss: 14377.2990, Time: 37.88s\n",
      "Estimated remaining time: 23.36 minutes\n",
      "Epoch 64 [1/4], Batch Loss: 449891.1250\n",
      "Epoch 64 [2/4], Batch Loss: 442053.3750\n",
      "Epoch 64 [3/4], Batch Loss: 483083.0938\n",
      "Epoch 64 [4/4], Batch Loss: 63836.0586\n",
      "Epoch 64, Average Loss: 14388.6365, Time: 36.97s\n",
      "Estimated remaining time: 22.18 minutes\n",
      "Epoch 65 [1/4], Batch Loss: 483772.2188\n",
      "Epoch 65 [2/4], Batch Loss: 429022.9062\n",
      "Epoch 65 [3/4], Batch Loss: 475712.1562\n",
      "Epoch 65 [4/4], Batch Loss: 49024.3555\n",
      "Epoch 65, Average Loss: 14375.3164, Time: 40.77s\n",
      "Estimated remaining time: 23.78 minutes\n",
      "Epoch 66 [1/4], Batch Loss: 472678.8125\n",
      "Epoch 66 [2/4], Batch Loss: 479489.2500\n",
      "Epoch 66 [3/4], Batch Loss: 430062.4062\n",
      "Epoch 66 [4/4], Batch Loss: 54069.6445\n",
      "Epoch 66, Average Loss: 14363.0011, Time: 39.10s\n",
      "Estimated remaining time: 22.16 minutes\n",
      "Epoch 67 [1/4], Batch Loss: 440234.2812\n",
      "Epoch 67 [2/4], Batch Loss: 443642.4688\n",
      "Epoch 67 [3/4], Batch Loss: 473323.4688\n",
      "Epoch 67 [4/4], Batch Loss: 83302.1797\n",
      "Epoch 67, Average Loss: 14405.0240, Time: 37.23s\n",
      "Estimated remaining time: 20.48 minutes\n",
      "Epoch 68 [1/4], Batch Loss: 468830.1875\n",
      "Epoch 68 [2/4], Batch Loss: 479225.3438\n",
      "Epoch 68 [3/4], Batch Loss: 436568.2188\n",
      "Epoch 68 [4/4], Batch Loss: 48335.1172\n",
      "Epoch 68, Average Loss: 14329.5887, Time: 36.71s\n",
      "Estimated remaining time: 19.58 minutes\n",
      "Epoch 69 [1/4], Batch Loss: 441375.0000\n",
      "Epoch 69 [2/4], Batch Loss: 470283.4688\n",
      "Epoch 69 [3/4], Batch Loss: 467326.0625\n",
      "Epoch 69 [4/4], Batch Loss: 54192.1719\n",
      "Epoch 69, Average Loss: 14331.7670, Time: 38.21s\n",
      "Estimated remaining time: 19.74 minutes\n",
      "Epoch 70 [1/4], Batch Loss: 463786.9688\n",
      "Epoch 70 [2/4], Batch Loss: 469754.0625\n",
      "Epoch 70 [3/4], Batch Loss: 442726.9688\n",
      "Epoch 70 [4/4], Batch Loss: 54416.5586\n",
      "Epoch 70, Average Loss: 14306.8456, Time: 39.91s\n",
      "Estimated remaining time: 19.95 minutes\n",
      "Epoch 71 [1/4], Batch Loss: 474698.7500\n",
      "Epoch 71 [2/4], Batch Loss: 454227.7188\n",
      "Epoch 71 [3/4], Batch Loss: 433352.7188\n",
      "Epoch 71 [4/4], Batch Loss: 67510.2344\n",
      "Epoch 71, Average Loss: 14297.8942, Time: 51.41s\n",
      "Estimated remaining time: 24.85 minutes\n",
      "Epoch 72 [1/4], Batch Loss: 450888.9062\n",
      "Epoch 72 [2/4], Batch Loss: 450784.4688\n",
      "Epoch 72 [3/4], Batch Loss: 469499.9375\n",
      "Epoch 72 [4/4], Batch Loss: 56603.3320\n",
      "Epoch 72, Average Loss: 14277.7664, Time: 38.87s\n",
      "Estimated remaining time: 18.14 minutes\n",
      "Epoch 73 [1/4], Batch Loss: 458717.4062\n",
      "Epoch 73 [2/4], Batch Loss: 472410.4062\n",
      "Epoch 73 [3/4], Batch Loss: 455225.8125\n",
      "Epoch 73 [4/4], Batch Loss: 44493.4219\n",
      "Epoch 73, Average Loss: 14308.4705, Time: 38.80s\n",
      "Estimated remaining time: 17.46 minutes\n",
      "Epoch 74 [1/4], Batch Loss: 454036.9375\n",
      "Epoch 74 [2/4], Batch Loss: 469825.5625\n",
      "Epoch 74 [3/4], Batch Loss: 443161.6562\n",
      "Epoch 74 [4/4], Batch Loss: 59417.8984\n",
      "Epoch 74, Average Loss: 14264.4205, Time: 44.73s\n",
      "Estimated remaining time: 19.38 minutes\n",
      "Epoch 75 [1/4], Batch Loss: 433580.9688\n",
      "Epoch 75 [2/4], Batch Loss: 445917.3438\n",
      "Epoch 75 [3/4], Batch Loss: 481407.2500\n",
      "Epoch 75 [4/4], Batch Loss: 67978.6250\n",
      "Epoch 75, Average Loss: 14288.8419, Time: 37.83s\n",
      "Estimated remaining time: 15.76 minutes\n",
      "Epoch 76 [1/4], Batch Loss: 480270.4688\n",
      "Epoch 76 [2/4], Batch Loss: 458990.2812\n",
      "Epoch 76 [3/4], Batch Loss: 426992.5000\n",
      "Epoch 76 [4/4], Batch Loss: 59411.7617\n",
      "Epoch 76, Average Loss: 14256.6501, Time: 38.39s\n",
      "Estimated remaining time: 15.35 minutes\n",
      "Epoch 77 [1/4], Batch Loss: 458168.8750\n",
      "Epoch 77 [2/4], Batch Loss: 457140.0312\n",
      "Epoch 77 [3/4], Batch Loss: 449805.7500\n",
      "Epoch 77 [4/4], Batch Loss: 58133.6133\n",
      "Epoch 77, Average Loss: 14232.4827, Time: 38.87s\n",
      "Estimated remaining time: 14.90 minutes\n",
      "Epoch 78 [1/4], Batch Loss: 472746.1562\n",
      "Epoch 78 [2/4], Batch Loss: 450821.8750\n",
      "Epoch 78 [3/4], Batch Loss: 452115.1250\n",
      "Epoch 78 [4/4], Batch Loss: 49638.1875\n",
      "Epoch 78, Average Loss: 14253.2134, Time: 39.13s\n",
      "Estimated remaining time: 14.35 minutes\n",
      "Epoch 79 [1/4], Batch Loss: 445771.7188\n",
      "Epoch 79 [2/4], Batch Loss: 458551.1562\n",
      "Epoch 79 [3/4], Batch Loss: 463456.8438\n",
      "Epoch 79 [4/4], Batch Loss: 54952.4766\n",
      "Epoch 79, Average Loss: 14227.3220, Time: 41.03s\n",
      "Estimated remaining time: 14.36 minutes\n",
      "Epoch 80 [1/4], Batch Loss: 459312.9688\n",
      "Epoch 80 [2/4], Batch Loss: 466297.0000\n",
      "Epoch 80 [3/4], Batch Loss: 443266.4375\n",
      "Epoch 80 [4/4], Batch Loss: 53719.1094\n",
      "Epoch 80, Average Loss: 14225.9552, Time: 40.03s\n",
      "Estimated remaining time: 13.34 minutes\n",
      "Epoch 81 [1/4], Batch Loss: 440214.3438\n",
      "Epoch 81 [2/4], Batch Loss: 448273.9688\n",
      "Epoch 81 [3/4], Batch Loss: 459904.8438\n",
      "Epoch 81 [4/4], Batch Loss: 73469.2109\n",
      "Epoch 81, Average Loss: 14218.6237, Time: 38.63s\n",
      "Estimated remaining time: 12.23 minutes\n",
      "Epoch 82 [1/4], Batch Loss: 427329.8750\n",
      "Epoch 82 [2/4], Batch Loss: 491393.5938\n",
      "Epoch 82 [3/4], Batch Loss: 440089.1875\n",
      "Epoch 82 [4/4], Batch Loss: 59955.8242\n",
      "Epoch 82, Average Loss: 14187.6848, Time: 38.16s\n",
      "Estimated remaining time: 11.45 minutes\n",
      "Epoch 83 [1/4], Batch Loss: 453625.0625\n",
      "Epoch 83 [2/4], Batch Loss: 465025.4062\n",
      "Epoch 83 [3/4], Batch Loss: 439639.2500\n",
      "Epoch 83 [4/4], Batch Loss: 62185.4531\n",
      "Epoch 83, Average Loss: 14204.7517, Time: 45.48s\n",
      "Estimated remaining time: 12.89 minutes\n",
      "Epoch 84 [1/4], Batch Loss: 464047.5938\n",
      "Epoch 84 [2/4], Batch Loss: 466607.6875\n",
      "Epoch 84 [3/4], Batch Loss: 429354.8125\n",
      "Epoch 84 [4/4], Batch Loss: 58063.4297\n",
      "Epoch 84, Average Loss: 14180.7352, Time: 38.44s\n",
      "Estimated remaining time: 10.25 minutes\n",
      "Epoch 85 [1/4], Batch Loss: 479519.7188\n",
      "Epoch 85 [2/4], Batch Loss: 427130.5625\n",
      "Epoch 85 [3/4], Batch Loss: 448457.7812\n",
      "Epoch 85 [4/4], Batch Loss: 64803.1641\n",
      "Epoch 85, Average Loss: 14199.1123, Time: 37.84s\n",
      "Estimated remaining time: 9.46 minutes\n",
      "Epoch 86 [1/4], Batch Loss: 467697.6250\n",
      "Epoch 86 [2/4], Batch Loss: 440386.4062\n",
      "Epoch 86 [3/4], Batch Loss: 443197.2812\n",
      "Epoch 86 [4/4], Batch Loss: 70064.3438\n",
      "Epoch 86, Average Loss: 14213.4566, Time: 38.65s\n",
      "Estimated remaining time: 9.02 minutes\n",
      "Epoch 87 [1/4], Batch Loss: 443849.8438\n",
      "Epoch 87 [2/4], Batch Loss: 456763.7500\n",
      "Epoch 87 [3/4], Batch Loss: 458210.1250\n",
      "Epoch 87 [4/4], Batch Loss: 56942.1641\n",
      "Epoch 87, Average Loss: 14157.6588, Time: 39.15s\n",
      "Estimated remaining time: 8.48 minutes\n",
      "Epoch 88 [1/4], Batch Loss: 453913.1562\n",
      "Epoch 88 [2/4], Batch Loss: 460864.8750\n",
      "Epoch 88 [3/4], Batch Loss: 425456.8125\n",
      "Epoch 88 [4/4], Batch Loss: 78723.4375\n",
      "Epoch 88, Average Loss: 14189.5828, Time: 43.07s\n",
      "Estimated remaining time: 8.61 minutes\n",
      "Epoch 89 [1/4], Batch Loss: 455154.4688\n",
      "Epoch 89 [2/4], Batch Loss: 435150.6875\n",
      "Epoch 89 [3/4], Batch Loss: 463382.3125\n",
      "Epoch 89 [4/4], Batch Loss: 58630.7969\n",
      "Epoch 89, Average Loss: 14123.1827, Time: 37.89s\n",
      "Estimated remaining time: 6.95 minutes\n",
      "Epoch 90 [1/4], Batch Loss: 461903.0000\n",
      "Epoch 90 [2/4], Batch Loss: 467740.9375\n",
      "Epoch 90 [3/4], Batch Loss: 424955.4062\n",
      "Epoch 90 [4/4], Batch Loss: 59677.4297\n",
      "Epoch 90, Average Loss: 14142.7677, Time: 38.01s\n",
      "Estimated remaining time: 6.34 minutes\n",
      "Epoch 91 [1/4], Batch Loss: 482281.8750\n",
      "Epoch 91 [2/4], Batch Loss: 427283.2500\n",
      "Epoch 91 [3/4], Batch Loss: 456120.3125\n",
      "Epoch 91 [4/4], Batch Loss: 49238.8906\n",
      "Epoch 91, Average Loss: 14149.2433, Time: 38.73s\n",
      "Estimated remaining time: 5.81 minutes\n",
      "Epoch 92 [1/4], Batch Loss: 439303.8438\n",
      "Epoch 92 [2/4], Batch Loss: 466532.7812\n",
      "Epoch 92 [3/4], Batch Loss: 442254.4375\n",
      "Epoch 92 [4/4], Batch Loss: 64052.9453\n",
      "Epoch 92, Average Loss: 14121.4401, Time: 39.32s\n",
      "Estimated remaining time: 5.24 minutes\n",
      "Epoch 93 [1/4], Batch Loss: 469587.5000\n",
      "Epoch 93 [2/4], Batch Loss: 460047.1562\n",
      "Epoch 93 [3/4], Batch Loss: 419581.9688\n",
      "Epoch 93 [4/4], Batch Loss: 64122.8242\n",
      "Epoch 93, Average Loss: 14133.3945, Time: 38.80s\n",
      "Estimated remaining time: 4.53 minutes\n",
      "Epoch 94 [1/4], Batch Loss: 445968.9688\n",
      "Epoch 94 [2/4], Batch Loss: 431875.0000\n",
      "Epoch 94 [3/4], Batch Loss: 473708.7812\n",
      "Epoch 94 [4/4], Batch Loss: 60313.9062\n",
      "Epoch 94, Average Loss: 14118.6666, Time: 40.42s\n",
      "Estimated remaining time: 4.04 minutes\n",
      "Epoch 95 [1/4], Batch Loss: 419877.3125\n",
      "Epoch 95 [2/4], Batch Loss: 480950.1875\n",
      "Epoch 95 [3/4], Batch Loss: 462306.1875\n",
      "Epoch 95 [4/4], Batch Loss: 48889.5117\n",
      "Epoch 95, Average Loss: 14120.2320, Time: 38.15s\n",
      "Estimated remaining time: 3.18 minutes\n",
      "Epoch 96 [1/4], Batch Loss: 441988.5312\n",
      "Epoch 96 [2/4], Batch Loss: 463963.2812\n",
      "Epoch 96 [3/4], Batch Loss: 440847.8125\n",
      "Epoch 96 [4/4], Batch Loss: 60328.2109\n",
      "Epoch 96, Average Loss: 14071.2784, Time: 39.21s\n",
      "Estimated remaining time: 2.61 minutes\n",
      "Epoch 97 [1/4], Batch Loss: 449538.3750\n",
      "Epoch 97 [2/4], Batch Loss: 457628.5938\n",
      "Epoch 97 [3/4], Batch Loss: 439556.1250\n",
      "Epoch 97 [4/4], Batch Loss: 61306.5742\n",
      "Epoch 97, Average Loss: 14080.2967, Time: 40.32s\n",
      "Estimated remaining time: 2.02 minutes\n",
      "Epoch 98 [1/4], Batch Loss: 441148.8438\n",
      "Epoch 98 [2/4], Batch Loss: 436822.1250\n",
      "Epoch 98 [3/4], Batch Loss: 454792.8750\n",
      "Epoch 98 [4/4], Batch Loss: 81388.9219\n",
      "Epoch 98, Average Loss: 14141.5277, Time: 39.15s\n",
      "Estimated remaining time: 1.30 minutes\n",
      "Epoch 99 [1/4], Batch Loss: 445651.4688\n",
      "Epoch 99 [2/4], Batch Loss: 435892.9062\n",
      "Epoch 99 [3/4], Batch Loss: 479225.2812\n",
      "Epoch 99 [4/4], Batch Loss: 48185.3555\n",
      "Epoch 99, Average Loss: 14089.5501, Time: 37.59s\n",
      "Estimated remaining time: 0.63 minutes\n",
      "Epoch 100 [1/4], Batch Loss: 441218.6250\n",
      "Epoch 100 [2/4], Batch Loss: 449367.1875\n",
      "Epoch 100 [3/4], Batch Loss: 450878.8750\n",
      "Epoch 100 [4/4], Batch Loss: 63742.9688\n",
      "Epoch 100, Average Loss: 14052.0766, Time: 41.76s\n",
      "Estimated remaining time: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# Define the VAE model\n",
    "control_patient_curv_model = VAE(input_shape=(1, ATLAS, ATLAS), hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM)\n",
    "\n",
    "# Train the VAE model\n",
    "train_vae(control_patient_curv_model, control_patient_curv_dataloader, epochs=EPOCHS, learning_rate=LR, device=DEVICE)\n",
    "# Optionally, save the entire model (including structure and state)\n",
    "torch.save(control_patient_curv_model, 'control_patient_curv_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_patient_adj_tensor = torch.concatenate((tensors['control_adj'], tensors['patient_adj']))\n",
    "# control_patient_adj_dataset = TensorDataset(control_patient_adj_tensor, control_patient_adj_tensor)\n",
    "# control_patient_adj_dataloader = DataLoader(control_patient_adj_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the VAE model\n",
    "# control_patient_adj_model = VAE(input_shape=(1, ATLAS, ATLAS), hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM)\n",
    "\n",
    "# # Train the VAE model\n",
    "# train_vae(control_patient_adj_model, control_patient_adj_dataloader, epochs=EPOCHS, learning_rate=LR, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_connectome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
