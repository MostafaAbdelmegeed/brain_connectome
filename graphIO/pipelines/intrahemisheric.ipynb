{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import sys\n",
    "sys.path.append('C:/Users/mosta/OneDrive - UNCG\\Academics/CSC 699 - Thesis/repos/brain_connectome/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "ATLAS = 116\n",
    "LR = 0.0001\n",
    "HIDDEN = 512\n",
    "BATCH = 32\n",
    "PTH = '../../data/ppmi_corr_116.pth'\n",
    "LAYERS = 128\n",
    "ATT_HEAD = 58\n",
    "TEST_SIZE = 0.2\n",
    "DROPOUT = 0.6\n",
    "N_CLASS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_correlation_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Normalize a correlation matrix to the range [-1, 1].\n",
    "    \n",
    "    :param matrix: A numpy array representing the correlation matrix.\n",
    "    :return: A normalized correlation matrix.\n",
    "    \"\"\"\n",
    "    max_val = np.max(matrix)\n",
    "    min_val = np.min(matrix)\n",
    "    normalized_matrix = 2 * (matrix - min_val) / (max_val - min_val) - 1\n",
    "    return normalized_matrix\n",
    "\n",
    "def split_adjacency_matrix(adj_matrix):\n",
    "    \"\"\"\n",
    "    Split the adjacency matrix into left and right hemisphere matrices.\n",
    "    \n",
    "    :param adj_matrix: The original adjacency matrix.\n",
    "    :return: Two adjacency matrices for left and right hemispheres.\n",
    "    \"\"\"\n",
    "    left_indices = [i for i in range(adj_matrix.shape[0]) if i % 2 == 0]\n",
    "    right_indices = [i for i in range(adj_matrix.shape[0]) if i % 2 != 0]\n",
    "    \n",
    "    left_adj = adj_matrix[np.ix_(left_indices, left_indices)]\n",
    "    right_adj = adj_matrix[np.ix_(right_indices, right_indices)]\n",
    "    \n",
    "    return left_adj, right_adj\n",
    "\n",
    "def construct_graph(correlation_matrix, threshold=0.5):\n",
    "    num_regions = correlation_matrix.shape[0]\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(num_regions):\n",
    "        G.add_node(i, strength=correlation_matrix[i,:].mean())\n",
    "    \n",
    "    for i in range(num_regions):\n",
    "        for j in range(i + 1, num_regions):\n",
    "            if abs(correlation_matrix[i, j]) > threshold:\n",
    "                G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "    \n",
    "    pyg_graph = from_networkx(G)\n",
    "    \n",
    "    x = torch.eye(num_regions, dtype=torch.float)  # Identity matrix as dummy features\n",
    "    \n",
    "    edge_attr = []\n",
    "    for u, v in G.edges():\n",
    "        edge_attr.append([G[u][v]['weight']])\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    pyg_graph.x = x\n",
    "    pyg_graph.edge_attr = edge_attr\n",
    "    pyg_graph.edge_index = torch.tensor(list(G.edges)).t().contiguous()  # Ensure correct shape\n",
    "    \n",
    "    return pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_dataset = torch.load(PTH)\n",
    "ppmi_data = ppmi_dataset['data']\n",
    "ppmi_labels = ppmi_dataset['class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_indices = [i for i in range(len(ppmi_labels)) if ppmi_labels[i] == 0]\n",
    "pt_indices = [i for i in range(len(ppmi_labels)) if ppmi_labels[i] == 2]\n",
    "pr_indices = [i for i in range(len(ppmi_labels)) if ppmi_labels[i] == 1]\n",
    "sw_indices = [i for i in range(len(ppmi_labels)) if ppmi_labels[i] == 3]\n",
    "indices = cn_indices + pt_indices + pr_indices + sw_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 116, 116), (113, 116, 116), (67, 116, 116), (14, 116, 116))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_data = ppmi_data[cn_indices].numpy()\n",
    "pt_data = ppmi_data[pt_indices].numpy()\n",
    "pr_data = ppmi_data[pr_indices].numpy()\n",
    "sw_data = ppmi_data[sw_indices].numpy()\n",
    "cn_data.shape, pt_data.shape, pr_data.shape, sw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn_data has NaN: False\n",
      "pt_data has NaN: False\n",
      "pr_data has NaN: False\n",
      "sw_data has NaN: False\n"
     ]
    }
   ],
   "source": [
    "cn_has_nan = np.isnan(cn_data).any()\n",
    "pt_has_nan = np.isnan(pt_data).any()\n",
    "pr_has_nan = np.isnan(pr_data).any()\n",
    "sw_has_nan = np.isnan(sw_data).any()\n",
    "\n",
    "print(\"cn_data has NaN:\", cn_has_nan)\n",
    "print(\"pt_data has NaN:\", pt_has_nan)\n",
    "print(\"pr_data has NaN:\", pr_has_nan)\n",
    "print(\"sw_data has NaN:\", sw_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cn_data.shape[0]):\n",
    "    cn_data[i] = normalize_correlation_matrix(cn_data[i])\n",
    "for i in range(pt_data.shape[0]):\n",
    "    pt_data[i] = normalize_correlation_matrix(pt_data[i])\n",
    "for i in range(pr_data.shape[0]):\n",
    "    pr_data[i] = normalize_correlation_matrix(pr_data[i])\n",
    "for i in range(sw_data.shape[0]):\n",
    "    sw_data[i] = normalize_correlation_matrix(sw_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 58, 58) (209, 58, 58)\n"
     ]
    }
   ],
   "source": [
    "lh_data = []\n",
    "rh_data = []\n",
    "for i in range(cn_data.shape[0]):\n",
    "    lh, rh = split_adjacency_matrix(cn_data[i])\n",
    "    lh_data.append(lh)\n",
    "    rh_data.append(rh)\n",
    "for i in range(pt_data.shape[0]):\n",
    "    lh, rh = split_adjacency_matrix(pt_data[i])\n",
    "    lh_data.append(lh)\n",
    "    rh_data.append(rh)\n",
    "for i in range(pr_data.shape[0]):\n",
    "    lh, rh = split_adjacency_matrix(pr_data[i])\n",
    "    lh_data.append(lh)\n",
    "    rh_data.append(rh)\n",
    "for i in range(sw_data.shape[0]):\n",
    "    lh, rh = split_adjacency_matrix(sw_data[i])\n",
    "    lh_data.append(lh)\n",
    "    rh_data.append(rh)\n",
    "lh_data = np.array(lh_data)\n",
    "rh_data = np.array(rh_data)\n",
    "print(lh_data.shape, rh_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 209\n"
     ]
    }
   ],
   "source": [
    "lh_graphs = [construct_graph(lh_data[i]) for i in range(lh_data.shape[0])]\n",
    "rh_graphs = [construct_graph(rh_data[i]) for i in range(rh_data.shape[0])]\n",
    "print(len(lh_graphs), len(rh_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "labels = ppmi_labels[indices].numpy()\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainGraphDataset(Dataset):\n",
    "    def __init__(self, root, graphs, labels, transform=None, pre_transform=None):\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "        super(BrainGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.graphs[idx]\n",
    "        data.y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_dataset = BrainGraphDataset(root='', graphs=lh_graphs, labels=labels)\n",
    "rh_dataset = BrainGraphDataset(root='', graphs=rh_graphs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "test_size = int(TEST_SIZE * len(lh_dataset)) \n",
    "train_size = len(lh_dataset) - test_size\n",
    "lh_train_dataset, lh_test_dataset = torch.utils.data.random_split(lh_dataset, [train_size, test_size])\n",
    "rh_train_dataset, rh_test_dataset = torch.utils.data.random_split(rh_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "lh_train_loader = DataLoader(lh_train_dataset, batch_size=BATCH, shuffle=True)  # Adjust batch_size as needed\n",
    "lh_test_loader = DataLoader(lh_test_dataset, batch_size=BATCH, shuffle=False)\n",
    "rh_train_loader = DataLoader(rh_train_dataset, batch_size=BATCH, shuffle=True)\n",
    "rh_test_loader = DataLoader(rh_test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads, dropout=DROPOUT)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=DROPOUT)\n",
    "        self.lin1 = Linear(hidden_channels, in_channels)\n",
    "        self.lin2 = Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.dropout(x, p=DROPOUT, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=DROPOUT, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = F.dropout(x, p=DROPOUT, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Found NaN in loss\")\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    gts = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=-1)\n",
    "            correct += int((pred == data.y).sum())\n",
    "            preds.append(pred.cpu().numpy())\n",
    "            gts.append(data.y.cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    gts = np.concatenate(gts, axis=0)\n",
    "    accuracy = accuracy_score(gts, preds)\n",
    "    precision = precision_score(gts, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(gts, preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(gts, preds, average='weighted')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GAT, Left Hemisphere Parameters: 64503078\n",
      "Model: GAT, Right Hemisphere Parameters: 64503078\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lh_model = GAT(in_channels=ATLAS//2, hidden_channels=HIDDEN, out_channels=N_CLASS, heads=ATT_HEAD).to(device)\n",
    "rh_model = GAT(in_channels=ATLAS//2, hidden_channels=HIDDEN, out_channels=N_CLASS, heads=ATT_HEAD).to(device)\n",
    "lh_num_parameters = sum(p.numel() for p in lh_model.parameters() if p.requires_grad)\n",
    "rh_num_parameters = sum(p.numel() for p in rh_model.parameters() if p.requires_grad)\n",
    "print(f\"Model: GAT, Left Hemisphere Parameters: {lh_num_parameters}\")\n",
    "print(f\"Model: GAT, Right Hemisphere Parameters: {rh_num_parameters}\")\n",
    "lh_optimizer = Adam(lh_model.parameters(), LR, weight_decay=5e-4)\n",
    "rh_optimizer = Adam(rh_model.parameters(), LR, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Test Acc(L/R): 0.5610/0.0732, Loss: 1.3640/1.4486, Pre: 0.3147/0.0054, Rec: 0.5610/0.0732, F1: 0.4032/0.0100\n",
      "Epoch: 001, Test Acc(L/R): 0.5610/0.6341, Loss: 1.3351/1.4021, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 002, Test Acc(L/R): 0.5610/0.6341, Loss: 1.2791/1.2971, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 003, Test Acc(L/R): 0.5610/0.6341, Loss: 1.2265/1.2215, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 004, Test Acc(L/R): 0.5610/0.6341, Loss: 1.2005/1.1847, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 005, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0995/1.1048, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 006, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1719/1.1211, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 007, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0785/1.0714, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 008, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1078/1.0433, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 009, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0632/1.1382, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 010, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0703/1.0505, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 011, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0359/1.1294, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 012, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0531/1.1011, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 013, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0522/1.1507, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 014, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1343/1.0773, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 015, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0295/1.0640, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 016, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0144/1.0855, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 017, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1299/1.0942, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 018, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0442/1.0565, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 019, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1247/1.0761, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 020, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0653/1.1259, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 021, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0213/1.1614, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 022, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0493/1.0499, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 023, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0355/1.0364, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 024, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0076/1.1057, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 025, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0364/1.0557, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 026, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0143/1.0254, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 027, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0776/1.0902, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 028, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0780/1.0922, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 029, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0981/1.0313, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 030, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0887/1.0422, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 031, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0411/1.0488, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 032, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0010/1.0412, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 033, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0521/1.0245, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 034, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0279/1.1111, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 035, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0860/1.0334, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 036, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0297/1.0604, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 037, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0678/1.0649, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 038, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0762/1.0035, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 039, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0862/1.1171, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 040, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0741/1.0505, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 041, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0764/1.0223, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 042, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0783/1.0482, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 043, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0968/1.0653, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 044, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0462/1.0436, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 045, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0291/1.0619, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 046, Test Acc(L/R): 0.5610/0.6341, Loss: 1.1015/1.0328, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 047, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0715/1.0917, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 048, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0433/1.0775, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n",
      "Epoch: 049, Test Acc(L/R): 0.5610/0.6341, Loss: 1.0391/1.0605, Pre: 0.3147/0.4021, Rec: 0.5610/0.6341, F1: 0.4032/0.4922\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    lh_loss = train(lh_model, lh_train_loader, criterion, lh_optimizer, device)\n",
    "    rh_loss = train(rh_model, rh_train_loader, criterion, rh_optimizer, device)\n",
    "    lh_test_acc, lh_pre, lh_rec, lh_f1 = test(lh_model, lh_test_loader, device)\n",
    "    rh_test_acc, rh_pre, rh_rec, rh_f1 = test(rh_model, rh_test_loader, device)\n",
    "    print(f'Epoch: {epoch:03d}, Test Acc(L/R): {lh_test_acc:.4f}/{rh_test_acc:.4f}, Loss: {lh_loss:.4f}/{rh_loss:.4f}, Pre: {lh_pre:.4f}/{rh_pre:.4f}, Rec: {lh_rec:.4f}/{rh_rec:.4f}, F1: {lh_f1:.4f}/{rh_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lh_model, f'../../ppmi_corr_lh_model_{LAYERS}_{HIDDEN}.pth')\n",
    "torch.save(rh_model, f'../../ppmi_corr_rh_model_{LAYERS}_{HIDDEN}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_head_gat_with_edge_features import GATWithEdgeFeatures\n",
    "\n",
    "lh_gat = GATWithEdgeFeatures(in_features=ATLAS//2, out_channels=N_CLASS, heads=ATT_HEAD).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_connectome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
