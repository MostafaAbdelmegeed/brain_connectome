{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from scipy.sparse import csr_matrix\n",
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = torch.load('../../ppmi.pth')\n",
    "data_tensor = loaded_data['data'].to(dtype=torch.float32)\n",
    "class_label = loaded_data['class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_correlation_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Normalize a correlation matrix to the range [-1, 1].\n",
    "    \n",
    "    :param matrix: A numpy array or PyTorch tensor representing the correlation matrix.\n",
    "    :return: A normalized correlation matrix of the same type (numpy array or PyTorch tensor).\n",
    "    \"\"\"\n",
    "    if isinstance(matrix, np.ndarray):\n",
    "        max_val = np.max(matrix)\n",
    "        min_val = np.min(matrix)\n",
    "        normalized_matrix = 2 * (matrix - min_val) / (max_val - min_val) - 1\n",
    "    elif isinstance(matrix, torch.Tensor):\n",
    "        max_val = torch.max(matrix)\n",
    "        min_val = torch.min(matrix)\n",
    "        normalized_matrix = 2 * (matrix - min_val) / (max_val - min_val) - 1\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a numpy array or a PyTorch tensor.\")\n",
    "    \n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pyg_graph(correlation_matrix, threshold=0.5):\n",
    "    num_regions = correlation_matrix.shape[0]\n",
    "    # Create NetworkX graph to utilize its easy manipulation capabilities\n",
    "    G = nx.Graph()\n",
    "    # Add nodes with alternating hemispheres based on AAL116 atlas\n",
    "    for i in range(num_regions):\n",
    "        hemisphere = 'left' if i % 2 == 0 else 'right'\n",
    "        G.add_node(i, hemisphere=hemisphere)\n",
    "    # Add edges\n",
    "    for i in range(num_regions):\n",
    "        for j in range(i + 1, num_regions):\n",
    "            if abs(correlation_matrix[i, j]) > threshold:\n",
    "                G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "    # Convert to PyTorch Geometric Data object\n",
    "    pyg_graph = from_networkx(G)\n",
    "    return pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_graphs(data_tensor, batch_size=10, threshold=0.5):\n",
    "    graphs = []\n",
    "    num_batches = (data_tensor.shape[0] + batch_size - 1) // batch_size\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, data_tensor.shape[0])\n",
    "        print(f\"Processing batch {batch_idx+1}/{num_batches}, matrices {start_idx+1} to {end_idx}\")\n",
    "        batch_graphs = []\n",
    "        for i in range(start_idx, end_idx):\n",
    "            print(f\"Processing matrix {i+1}\")\n",
    "            if torch.isnan(data_tensor[i]).any() or torch.isinf(data_tensor[i]).any():\n",
    "                print(f\"Matrix {i+1} contains NaN or Inf values. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                normalized_matrix = normalize_correlation_matrix(data_tensor[i].to(dtype=torch.float32))  # Ensure float32\n",
    "                graph = construct_pyg_graph(normalized_matrix, threshold)\n",
    "                batch_graphs.append(graph)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing matrix {i+1}: {e}\")\n",
    "        graphs.extend(batch_graphs)\n",
    "        # Clear batch variables to free memory\n",
    "        del batch_graphs\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory if using GPU\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/209, matrices 1 to 1\n",
      "Processing matrix 1\n",
      "Processing batch 2/209, matrices 2 to 2\n",
      "Processing matrix 2\n",
      "Processing batch 3/209, matrices 3 to 3\n",
      "Processing matrix 3\n",
      "Processing batch 4/209, matrices 4 to 4\n",
      "Processing matrix 4\n",
      "Processing batch 5/209, matrices 5 to 5\n",
      "Processing matrix 5\n",
      "Processing batch 6/209, matrices 6 to 6\n",
      "Processing matrix 6\n",
      "Processing batch 7/209, matrices 7 to 7\n",
      "Processing matrix 7\n",
      "Processing batch 8/209, matrices 8 to 8\n",
      "Processing matrix 8\n",
      "Processing batch 9/209, matrices 9 to 9\n",
      "Processing matrix 9\n",
      "Processing batch 10/209, matrices 10 to 10\n",
      "Processing matrix 10\n",
      "Processing batch 11/209, matrices 11 to 11\n",
      "Processing matrix 11\n",
      "Processing batch 12/209, matrices 12 to 12\n",
      "Processing matrix 12\n",
      "Processing batch 13/209, matrices 13 to 13\n",
      "Processing matrix 13\n",
      "Processing batch 14/209, matrices 14 to 14\n",
      "Processing matrix 14\n",
      "Processing batch 15/209, matrices 15 to 15\n",
      "Processing matrix 15\n",
      "Processing batch 16/209, matrices 16 to 16\n",
      "Processing matrix 16\n",
      "Processing batch 17/209, matrices 17 to 17\n",
      "Processing matrix 17\n",
      "Processing batch 18/209, matrices 18 to 18\n",
      "Processing matrix 18\n",
      "Processing batch 19/209, matrices 19 to 19\n",
      "Processing matrix 19\n",
      "Processing batch 20/209, matrices 20 to 20\n",
      "Processing matrix 20\n",
      "Processing batch 21/209, matrices 21 to 21\n",
      "Processing matrix 21\n",
      "Processing batch 22/209, matrices 22 to 22\n",
      "Processing matrix 22\n",
      "Processing batch 23/209, matrices 23 to 23\n",
      "Processing matrix 23\n",
      "Processing batch 24/209, matrices 24 to 24\n",
      "Processing matrix 24\n",
      "Processing batch 25/209, matrices 25 to 25\n",
      "Processing matrix 25\n",
      "Processing batch 26/209, matrices 26 to 26\n",
      "Processing matrix 26\n",
      "Processing batch 27/209, matrices 27 to 27\n",
      "Processing matrix 27\n",
      "Processing batch 28/209, matrices 28 to 28\n",
      "Processing matrix 28\n",
      "Processing batch 29/209, matrices 29 to 29\n",
      "Processing matrix 29\n",
      "Processing batch 30/209, matrices 30 to 30\n",
      "Processing matrix 30\n",
      "Processing batch 31/209, matrices 31 to 31\n",
      "Processing matrix 31\n",
      "Processing batch 32/209, matrices 32 to 32\n",
      "Processing matrix 32\n",
      "Processing batch 33/209, matrices 33 to 33\n",
      "Processing matrix 33\n",
      "Processing batch 34/209, matrices 34 to 34\n",
      "Processing matrix 34\n",
      "Processing batch 35/209, matrices 35 to 35\n",
      "Processing matrix 35\n",
      "Processing batch 36/209, matrices 36 to 36\n",
      "Processing matrix 36\n",
      "Processing batch 37/209, matrices 37 to 37\n",
      "Processing matrix 37\n",
      "Processing batch 38/209, matrices 38 to 38\n",
      "Processing matrix 38\n",
      "Processing batch 39/209, matrices 39 to 39\n",
      "Processing matrix 39\n",
      "Processing batch 40/209, matrices 40 to 40\n",
      "Processing matrix 40\n",
      "Processing batch 41/209, matrices 41 to 41\n",
      "Processing matrix 41\n",
      "Processing batch 42/209, matrices 42 to 42\n",
      "Processing matrix 42\n",
      "Processing batch 43/209, matrices 43 to 43\n",
      "Processing matrix 43\n",
      "Processing batch 44/209, matrices 44 to 44\n",
      "Processing matrix 44\n",
      "Processing batch 45/209, matrices 45 to 45\n",
      "Processing matrix 45\n",
      "Processing batch 46/209, matrices 46 to 46\n",
      "Processing matrix 46\n",
      "Processing batch 47/209, matrices 47 to 47\n",
      "Processing matrix 47\n",
      "Processing batch 48/209, matrices 48 to 48\n",
      "Processing matrix 48\n",
      "Processing batch 49/209, matrices 49 to 49\n",
      "Processing matrix 49\n",
      "Processing batch 50/209, matrices 50 to 50\n",
      "Processing matrix 50\n",
      "Processing batch 51/209, matrices 51 to 51\n",
      "Processing matrix 51\n",
      "Processing batch 52/209, matrices 52 to 52\n",
      "Processing matrix 52\n",
      "Processing batch 53/209, matrices 53 to 53\n",
      "Processing matrix 53\n",
      "Processing batch 54/209, matrices 54 to 54\n",
      "Processing matrix 54\n",
      "Processing batch 55/209, matrices 55 to 55\n",
      "Processing matrix 55\n",
      "Processing batch 56/209, matrices 56 to 56\n",
      "Processing matrix 56\n",
      "Processing batch 57/209, matrices 57 to 57\n",
      "Processing matrix 57\n",
      "Processing batch 58/209, matrices 58 to 58\n",
      "Processing matrix 58\n",
      "Processing batch 59/209, matrices 59 to 59\n",
      "Processing matrix 59\n",
      "Processing batch 60/209, matrices 60 to 60\n",
      "Processing matrix 60\n",
      "Processing batch 61/209, matrices 61 to 61\n",
      "Processing matrix 61\n",
      "Processing batch 62/209, matrices 62 to 62\n",
      "Processing matrix 62\n",
      "Processing batch 63/209, matrices 63 to 63\n",
      "Processing matrix 63\n",
      "Processing batch 64/209, matrices 64 to 64\n",
      "Processing matrix 64\n",
      "Processing batch 65/209, matrices 65 to 65\n",
      "Processing matrix 65\n",
      "Processing batch 66/209, matrices 66 to 66\n",
      "Processing matrix 66\n",
      "Processing batch 67/209, matrices 67 to 67\n",
      "Processing matrix 67\n",
      "Processing batch 68/209, matrices 68 to 68\n",
      "Processing matrix 68\n",
      "Processing batch 69/209, matrices 69 to 69\n",
      "Processing matrix 69\n",
      "Processing batch 70/209, matrices 70 to 70\n",
      "Processing matrix 70\n",
      "Processing batch 71/209, matrices 71 to 71\n",
      "Processing matrix 71\n",
      "Processing batch 72/209, matrices 72 to 72\n",
      "Processing matrix 72\n",
      "Processing batch 73/209, matrices 73 to 73\n",
      "Processing matrix 73\n",
      "Processing batch 74/209, matrices 74 to 74\n",
      "Processing matrix 74\n",
      "Processing batch 75/209, matrices 75 to 75\n",
      "Processing matrix 75\n",
      "Processing batch 76/209, matrices 76 to 76\n",
      "Processing matrix 76\n",
      "Processing batch 77/209, matrices 77 to 77\n",
      "Processing matrix 77\n",
      "Processing batch 78/209, matrices 78 to 78\n",
      "Processing matrix 78\n",
      "Processing batch 79/209, matrices 79 to 79\n",
      "Processing matrix 79\n",
      "Processing batch 80/209, matrices 80 to 80\n",
      "Processing matrix 80\n",
      "Processing batch 81/209, matrices 81 to 81\n",
      "Processing matrix 81\n",
      "Processing batch 82/209, matrices 82 to 82\n",
      "Processing matrix 82\n",
      "Processing batch 83/209, matrices 83 to 83\n",
      "Processing matrix 83\n",
      "Processing batch 84/209, matrices 84 to 84\n",
      "Processing matrix 84\n",
      "Processing batch 85/209, matrices 85 to 85\n",
      "Processing matrix 85\n",
      "Processing batch 86/209, matrices 86 to 86\n",
      "Processing matrix 86\n",
      "Processing batch 87/209, matrices 87 to 87\n",
      "Processing matrix 87\n",
      "Processing batch 88/209, matrices 88 to 88\n",
      "Processing matrix 88\n",
      "Processing batch 89/209, matrices 89 to 89\n",
      "Processing matrix 89\n",
      "Processing batch 90/209, matrices 90 to 90\n",
      "Processing matrix 90\n",
      "Processing batch 91/209, matrices 91 to 91\n",
      "Processing matrix 91\n",
      "Processing batch 92/209, matrices 92 to 92\n",
      "Processing matrix 92\n",
      "Processing batch 93/209, matrices 93 to 93\n",
      "Processing matrix 93\n",
      "Processing batch 94/209, matrices 94 to 94\n",
      "Processing matrix 94\n",
      "Processing batch 95/209, matrices 95 to 95\n",
      "Processing matrix 95\n",
      "Processing batch 96/209, matrices 96 to 96\n",
      "Processing matrix 96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m graphs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mprocess_data_in_batches\u001b[1;34m(data_tensor, batch_size, threshold)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     normalized_matrix \u001b[38;5;241m=\u001b[39m normalize_correlation_matrix(data_tensor[i])\n\u001b[1;32m---> 18\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_pyg_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     batch_graphs\u001b[38;5;241m.\u001b[39mappend(graph)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mconstruct_pyg_graph\u001b[1;34m(correlation_matrix, threshold)\u001b[0m\n\u001b[0;32m     13\u001b[0m             G\u001b[38;5;241m.\u001b[39madd_edge(i, j, weight\u001b[38;5;241m=\u001b[39mcorrelation_matrix[i, j])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch Geometric Data object\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m pyg_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pyg_graph\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\site-packages\\torch_geometric\\utils\\convert.py:229\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m--> 229\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_directed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mis_directed(G) \u001b[38;5;28;01melse\u001b[39;00m G\n\u001b[0;32m    231\u001b[0m mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes(), \u001b[38;5;28mrange\u001b[39m(G\u001b[38;5;241m.\u001b[39mnumber_of_nodes())))\n\u001b[0;32m    232\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m2\u001b[39m, G\u001b[38;5;241m.\u001b[39mnumber_of_edges()), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\site-packages\\networkx\\classes\\graph.py:1699\u001b[0m, in \u001b[0;36mGraph.to_directed\u001b[1;34m(self, as_view)\u001b[0m\n\u001b[0;32m   1697\u001b[0m G\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph))\n\u001b[0;32m   1698\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from((n, deepcopy(d)) \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m-> 1699\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\site-packages\\networkx\\classes\\digraph.py:768\u001b[0m, in \u001b[0;36mDiGraph.add_edges_from\u001b[1;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_edges_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, ebunch_to_add, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[0;32m    714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add all the edges in ebunch_to_add.\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m    >>> G.add_edges_from(list((5, n) for n in G.nodes))\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ebunch_to_add:\n\u001b[0;32m    769\u001b[0m         ne \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(e)\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ne \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\site-packages\\networkx\\classes\\graph.py:1700\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1697\u001b[0m G\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph))\n\u001b[0;32m   1698\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from((n, deepcopy(d)) \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1699\u001b[0m G\u001b[38;5;241m.\u001b[39madd_edges_from(\n\u001b[1;32m-> 1700\u001b[0m     (u, v, \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m u, nbrs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, data \u001b[38;5;129;01min\u001b[39;00m nbrs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1703\u001b[0m )\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mosta\\miniconda3\\envs\\brain_connectome\\lib\\site-packages\\torch\\_tensor.py:182\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensor) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for non-wrapper subclasses \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly works for subclass types that implement new_empty() and for which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man instance of a different type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m \u001b[43mnew_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_conj():\n\u001b[0;32m    186\u001b[0m     new_tensor \u001b[38;5;241m=\u001b[39m new_tensor\u001b[38;5;241m.\u001b[39mconj_physical()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graphs = adj_to_graphs(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interhemispheric_subgraph(pyg_graph):\n",
    "    G = to_networkx(pyg_graph)\n",
    "    subgraph = nx.Graph()\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if G.nodes[u]['hemisphere'] != G.nodes[v]['hemisphere']:\n",
    "            subgraph.add_edge(u, v, weight=data['weight'])\n",
    "    pyg_subgraph = from_networkx(subgraph)\n",
    "    return pyg_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emphasize_interhemispheric_edges(pyg_graph, emphasis_factor=2):\n",
    "    pyg_graph = pyg_graph.clone()  # Clone to avoid modifying the original graph\n",
    "    G = to_networkx(pyg_graph)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if G.nodes[u]['hemisphere'] != G.nodes[v]['hemisphere']:\n",
    "            G[u][v]['weight'] *= emphasis_factor\n",
    "    emphasized_pyg_graph = from_networkx(G)\n",
    "    return emphasized_pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_correlation_matrices(correlation_matrices, threshold=0.5, emphasis_factor=2):\n",
    "    graphs = []\n",
    "    interhemispheric_subgraphs = []\n",
    "    emphasized_graphs = []\n",
    "    \n",
    "    for matrix in correlation_matrices:\n",
    "        pyg_graph = construct_pyg_graph(matrix, threshold)\n",
    "        subgraph = extract_interhemispheric_subgraph(pyg_graph)\n",
    "        emphasized_graph = emphasize_interhemispheric_edges(pyg_graph, emphasis_factor)\n",
    "        \n",
    "        graphs.append(pyg_graph)\n",
    "        interhemispheric_subgraphs.append(subgraph)\n",
    "        emphasized_graphs.append(emphasized_graph)\n",
    "    \n",
    "    return graphs, interhemispheric_subgraphs, emphasized_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Assuming correlation_matrices is loaded from some source, e.g., torch.load or np.load\n",
    "    correlation_matrices = np.load('./data/correlation_matrices.npy')\n",
    "    \n",
    "    train_size = int(0.7 * len(correlation_matrices))\n",
    "    val_size = int(0.15 * len(correlation_matrices))\n",
    "    test_size = len(correlation_matrices) - train_size - val_size\n",
    "    \n",
    "    train_matrices, val_matrices, test_matrices = random_split(correlation_matrices, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_graphs, _, _ = process_correlation_matrices(train_matrices)\n",
    "    val_graphs, _, _ = process_correlation_matrices(val_matrices)\n",
    "    test_graphs, _, _ = process_correlation_matrices(test_matrices)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    set_seeds()\n",
    "    device = get_device(args.gpu)\n",
    "    train_loader, val_loader, test_loader = load_data()\n",
    "    model = get_model(args.model, args.hidden_channels).to(device)\n",
    "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model: {args.model}, parameters: {num_parameters}\")\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_acc, _, _, _ = test(model, val_loader, device)\n",
    "        test_acc, pre, rec, f1 = test(model, test_loader, device)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        print(f'Epoch: {epoch:03d}, best Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}, Loss: {loss:.4f}, pre: {pre:.4f}, rec: {rec:.4f}, f1: {f1:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_connectome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
